{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPieceTokenizer:\n",
    "    def __init__(self,corpus_file_path,path_for_vocab_txt_out,sentences_id_in_json,token_id_out_json,iterations,max_vocab_size):\n",
    "        self.corpus_file_path=corpus_file_path\n",
    "        self.iterations=iterations\n",
    "        self.vocabulary=None\n",
    "        self.path_for_vocab_txt_out=path_for_vocab_txt_out\n",
    "        self.max_vocab_size=max_vocab_size\n",
    "        self.sentences_id_in_json=sentences_id_in_json\n",
    "        self.token_id_out_json=token_id_out_json\n",
    "        with open(corpus_file_path, 'r') as file:\n",
    "            self.corpus=file.read()\n",
    "        self.corpus=self.corpus\n",
    "    def extract_words(self):\n",
    "        words_freq={}\n",
    "        word=\"\"\n",
    "        for character in self.corpus:\n",
    "            if 'a'<=character and character<='z':\n",
    "                word+=character\n",
    "            elif( character>= \"!\" and character<= \"/\") or( character>= \":\" and character<=\"?\"):\n",
    "                if character not in words_freq:\n",
    "                    words_freq[character]=1\n",
    "                      \n",
    "                else:\n",
    "                    words_freq[character]+=1\n",
    "                   \n",
    "\n",
    "                if word  != \"\":\n",
    "                    if word not in words_freq:\n",
    "                        words_freq[word]=1\n",
    "                        word=\"\"\n",
    "                    else:\n",
    "                        words_freq[word]+=1\n",
    "                        word=\"\"\n",
    "            else:\n",
    "                if word  != \"\":\n",
    "                    if word not in words_freq:\n",
    "                        words_freq[word]=1\n",
    "                        word=\"\"\n",
    "                    else:\n",
    "                        words_freq[word]+=1\n",
    "                        word=\"\"\n",
    "        if word != \"\":\n",
    "            if word not in words_freq:\n",
    "                words_freq[word]=1\n",
    "            else:\n",
    "                words_freq[word]+= 1\n",
    "\n",
    "\n",
    "        print(words_freq)\n",
    "\n",
    "        word_splits={}\n",
    "        for i in words_freq.keys():\n",
    "            word_temp=[]\n",
    "            o=True\n",
    "            for k in i:\n",
    "                if o:\n",
    "                    word_temp.append(k)\n",
    "                    o=False\n",
    "                else:\n",
    "                    word_temp.append(\"##\"+k)\n",
    "            word_splits[i]=word_temp\n",
    "\n",
    "\n",
    "        print(word_splits,words_freq)\n",
    "        return word_splits,words_freq\n",
    "    def init_vocab(self, word_split):\n",
    "        vocab_init=[\"[PAD]\",\"[UNK]\"]\n",
    "        for i,j in word_split.items():\n",
    "            for k in j:\n",
    "                if k not in vocab_init:\n",
    "                    vocab_init.append(k)\n",
    "        return vocab_init\n",
    "    def compute_pair_score(self, words_freq, word_splits):\n",
    "        pair_freq=defaultdict(int)\n",
    "        letter_freq=defaultdict(int)\n",
    "        for word,freq in words_freq.items():\n",
    "            split_of_word=word_splits[word]\n",
    "            if(len(split_of_word)==1):\n",
    "                letter_freq[split_of_word[0]]+=1\n",
    "            else:\n",
    "                for i in range(len(split_of_word)-1):\n",
    "                    pair_of_split=(split_of_word[i],split_of_word[i+1])\n",
    "\n",
    "                    letter_freq[split_of_word[i]]+=freq\n",
    "\n",
    "                    pair_freq[pair_of_split]+=freq\n",
    "\n",
    "                letter_freq[split_of_word[-1]]+=freq\n",
    "        scores={}\n",
    "        for pair_word,freq in pair_freq.items():\n",
    "            scores[pair_word]=freq/(letter_freq[pair_word[0]]*letter_freq[pair_word[1]])\n",
    "        return scores\n",
    "    def merge_pair(self,a,b, splits):\n",
    "        for word_temp, split_temp in splits.items():\n",
    "            split_temp_loop=split_temp\n",
    "            if len(split_temp_loop)>1:\n",
    "                i=0\n",
    "                while i<len(split_temp_loop)-1:\n",
    "                    if split_temp_loop[i]==a and split_temp_loop[i+1]==b:\n",
    "                        merge=\"\"\n",
    "                        if b.startswith(\"##\"):\n",
    "                            merge=a+b[2:]\n",
    "                        else:\n",
    "                            merge=a+b\n",
    "                        split_temp_loop=split_temp_loop[:i]+[merge]+split_temp_loop[i+2:]\n",
    "                    else:\n",
    "                        i+=1\n",
    "                splits[word_temp]=split_temp_loop\n",
    "        return splits\n",
    "    def construct_vocabulary(self):\n",
    "        word_splits,words_freq=self.extract_words()\n",
    "        vocab_temp=self.init_vocab(word_splits)\n",
    "        for i in range(self.iterations):\n",
    "            score_temp=self.compute_pair_score(words_freq,word_splits)\n",
    "            best_pair=None\n",
    "            max_score=None\n",
    "            for pair_temp,prob_temp in score_temp.items():\n",
    "                if best_pair is None or max_score is None or max_score<prob_temp:\n",
    "                    best_pair=pair_temp\n",
    "                    max_score=prob_temp\n",
    "            if best_pair==None or max_score==None:\n",
    "                break\n",
    "            word_splits=self.merge_pair(best_pair[0],best_pair[1],word_splits)\n",
    "            \n",
    "            token_to_insert=\"\"\n",
    "            if(best_pair[1][0:2]==\"##\"):\n",
    "                token_to_insert=best_pair[0]+best_pair[1][2:]\n",
    "            else:\n",
    "                token_to_insert=best_pair[0]+best_pair[1]\n",
    "            if token_to_insert not in vocab_temp:\n",
    "                vocab_temp.append(token_to_insert)\n",
    "        print(\"__________\")\n",
    "        # self.vocabulary.sort()\n",
    "        self.vocabulary=vocab_temp\n",
    "        self.vocabulary.sort()\n",
    "\n",
    "        self.write_vocab_to_file()\n",
    "        return vocab_temp\n",
    "    def write_vocab_to_file(self):\n",
    "        with open(self.path_for_vocab_txt_out,\"w\") as file:\n",
    "            for tokens in self.vocabulary:\n",
    "                file.write(tokens+\"\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "    def extract_from_json(self,path_to_json):\n",
    "        with open(path_to_json,\"r\") as f:\n",
    "            test_data=json.load(f)\n",
    "        return test_data\n",
    "\n",
    "\n",
    "\n",
    "    def tokenize_word(self,word_to_tokenize):\n",
    "        tokens_res=[]\n",
    "        start_ind=0\n",
    "        while start_ind<len(word_to_tokenize):\n",
    "            end_ind=len(word_to_tokenize)\n",
    "\n",
    "            found_token=False\n",
    "\n",
    "            while start_ind<end_ind:\n",
    "                subword_to_find=word_to_tokenize[start_ind:end_ind]\n",
    "                if start_ind>0:\n",
    "                    subword_to_find=\"##\"+subword_to_find\n",
    "\n",
    "                if subword_to_find in self.vocabulary:\n",
    "                    tokens_res.append(subword_to_find)\n",
    "                    start_ind=end_ind\n",
    "                    found_token=True\n",
    "                    break\n",
    "                end_ind-=1\n",
    "            if not found_token:\n",
    "                tokens_res.append(\"[UNK]\")\n",
    "                start_ind+=1\n",
    "        return tokens_res\n",
    "\n",
    "    def tokenize_sentence(self, text_to_tokenize):\n",
    "        sentence_to_tokenize=[]\n",
    "        word=\"\"\n",
    "        for character in text_to_tokenize:\n",
    "            if 'a'<=character and character<='z':\n",
    "                word+=character\n",
    "            else:\n",
    "                if word  != \"\":\n",
    "                    sentence_to_tokenize.append(word)\n",
    "                    word=\"\"\n",
    "                    \n",
    "        if word != \"\":\n",
    "            sentence_to_tokenize.append(word)\n",
    "        res_tokens_divided=[]\n",
    "        for word_to_tokenize in sentence_to_tokenize:\n",
    "            temp=[]\n",
    "            temp=self.tokenize_word(word_to_tokenize)\n",
    "            res_tokens_divided=res_tokens_divided+temp\n",
    "        return res_tokens_divided\n",
    "\n",
    "\n",
    "\n",
    "    def tokenize(self):\n",
    "        test_data=self.extract_from_json(self.sentences_id_in_json)\n",
    "        data_to_be_written={}\n",
    "        for entry in test_data:\n",
    "            id_sen=entry[\"id\"]\n",
    "            sen=entry[\"sentence\"]\n",
    "            data_to_be_written[id_sen]=self.tokenize_sentence(sen[:-1])\n",
    "\n",
    "\n",
    "    \n",
    "        out_file=self.token_id_out_json\n",
    "        with open(out_file,\"w\") as f:\n",
    "            json.dump(data_to_be_written,f,indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_class=WordPieceTokenizer(\"corpus.txt\",\"vocabulary_out.txt\",\"sample_test.json\",\"sentences_to_token_file.json\",100000,0)\n",
    "test_class.construct_vocabulary()\n",
    "test_class.tokenize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
